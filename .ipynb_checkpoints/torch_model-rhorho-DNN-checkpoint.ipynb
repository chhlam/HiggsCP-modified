{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87e49442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/anderson/anaconda3/envs/py3/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from Eearly_stop import *\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import sys\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from src_py.cpmix_utils import preprocess_data\n",
    "from src_py.rhorho import RhoRhoEvent\n",
    "from src_py.a1a1 import A1A1Event\n",
    "from src_py.a1rho import A1RhoEvent\n",
    "from src_py.data_utils import read_np, EventDatasets\n",
    "from src_py.process_background import convert_bkgd_raw\n",
    "import train_rhorho, train_a1rho, train_a1a1\n",
    "from src_py.metrics_utils import calculate_deltas_unsigned, calculate_deltas_signed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5e91a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da403d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decaymodes = ['rhorho', 'a1rho', 'a1a1']\n",
    "types = {\"nn_rhorho\": train_rhorho.start,\"nn_a1rho\": train_a1rho.start,\"nn_a1a1\": train_a1a1.start}\n",
    "parser = argparse.ArgumentParser(description='Train classifier')\n",
    "\n",
    "parser.add_argument(\"--num_classes\", dest=\"NUM_CLASSES\", type=int, default=31)\n",
    "parser.add_argument(\"-l\", \"--layers\", dest=\"LAYERS\", type=int, help = \"number of NN layers\", default=6)\n",
    "parser.add_argument(\"-s\", \"--size\", dest=\"SIZE\", type=int, help=\"NN size\", default=100)\n",
    "parser.add_argument(\"-lambda\", \"--lambda\", type=float, dest=\"LAMBDA\", help=\"value of lambda parameter\", default=0.0)\n",
    "parser.add_argument(\"-m\", \"--method\", dest=\"METHOD\", choices=[\"A\", \"B\", \"C\"], default=\"A\")\n",
    "parser.add_argument(\"-o\", \"--optimizer\", dest=\"OPT\", \n",
    "    choices=[\"GradientDescentOptimizer\", \"AdadeltaOptimizer\", \"AdagradOptimizer\",\n",
    "         \"ProximalAdagradOptimizer\", \"AdamOptimizer\", \"FtrlOptimizer\",\n",
    "         \"ProximalGradientDescentOptimizer\", \"RMSPropOptimizer\"], default=\"AdamOptimizer\")\n",
    "parser.add_argument(\"-d\", \"--dropout\", dest=\"DROPOUT\", type=float, default=0.0)\n",
    "parser.add_argument(\"-e\", \"--epochs\", dest=\"EPOCHS\", type=int, default=3)\n",
    "# parser.add_argument(\"-f\", \"--features\", dest=\"FEAT\", help=\"Features\", default=\"Variant-All\")\n",
    "# #         choices= [\"Variant-All\", \"Variant-1.0\", \"Variant-1.1\", \"Variant-2.0\", \"Variant-2.1\",\n",
    "# #                   \"Variant-2.2\", \"Variant-3.0\", \"Variant-3.1\", \"Variant-4.0\", \"Variant-4.1\"])\n",
    "\n",
    "########### Change this to according dir to download data #######################\n",
    "parser.add_argument(\"-i\", \"--input\", dest=\"IN\", default='/home/anderson/higgs_data/rhorho')\n",
    "\n",
    "parser.add_argument(\"--miniset\", dest=\"MINISET\", type=lambda s: s.lower() in ['true', 't', 'yes', '1'], default=False)\n",
    "parser.add_argument(\"--z_noise_fraction\", dest=\"Z_NOISE_FRACTION\", type=float, default=0.0)\n",
    "\n",
    "parser.add_argument(\"--delt_classes\", dest=\"DELT_CLASSES\", type=int, default=0,\n",
    "                    help='Maximal distance between predicted and valid class for event being considered as correctly classified')\n",
    "\n",
    "parser.add_argument(\"--unweighted\", dest=\"UNWEIGHTED\", type=lambda s: s.lower() in ['true', 't', 'yes', '1'], default=False)\n",
    "parser.add_argument(\"--reuse_weights\", dest=\"REUSE_WEIGHTS\", type=bool, default=False)\n",
    "parser.add_argument(\"--restrict_most_probable_angle\", dest=\"RESTRICT_MOST_PROBABLE_ANGLE\", type=bool, default=False)\n",
    "parser.add_argument(\"--force_download\", dest=\"FORCE_DOWNLOAD\", type=bool, default=False)\n",
    "parser.add_argument(\"--normalize_weights\", dest=\"NORMALIZE_WEIGHTS\", type=bool, default=False)\n",
    "\n",
    "\n",
    "parser.add_argument(\"--beta\",  type=float, dest=\"BETA\", help=\"value of beta parameter for polynomial smearing\", default=0.0)\n",
    "parser.add_argument(\"--pol_b\", type=float, dest=\"pol_b\", help=\"value of b parameter for polynomial smearing\", default=0.0)\n",
    "parser.add_argument(\"--pol_c\", type=float, dest=\"pol_c\", help=\"value of c parameter for polynomial smearing\", default=0.0)\n",
    "\n",
    "parser.add_argument(\"--w1\", dest=\"W1\")\n",
    "parser.add_argument(\"--w2\", dest=\"W2\")\n",
    "parser.add_argument(\"--f\", dest=\"FEAT\", default=\"Variant-All\")\n",
    "parser.add_argument(\"--plot_features\", dest=\"PLOT_FEATURES\", choices=[\"NO\", \"FILTER\", \"NO-FILTER\"], default=\"NO\")\n",
    "parser.add_argument(\"--training_method\", dest=\"TRAINING_METHOD\", choices=[\"soft_weights\", \"soft_c012s\",  \"soft_argmaxs\", \"regr_c012s\", \"regr_weights\", \"regr_argmaxs\"], default=\"soft_weights\")\n",
    "parser.add_argument(\"--hits_c012s\", dest=\"HITS_C012s\", choices=[\"hits_c0s\", \"hits_c1s\",  \"hits_c2s\"], default=\"hits_c0s\")\n",
    "\n",
    "######Change this to according type (rhorho, a1rho, a1a1)#######################\n",
    "parser.add_argument(\"-t\", \"--type\", dest=\"TYPE\", choices=types.keys(), default='nn_rhorho')\n",
    "\n",
    "parser.add_argument(\"-r\", \"--reprocess\", dest=\"REPRO\", type=bool, default=True)\n",
    "args, unknown = parser.parse_known_args()\n",
    "parser.add_argument(\"-bkgd\", \"--bkgdpath\", dest=\"BKGDPATH\", default='/home/anderson/Ztt_raw/pythia.Z_115_135.%s.1M.*.outTUPLE_labFrame')\n",
    "args, unknown = parser.parse_known_args()\n",
    "parser.add_argument(\"--train_bkgd\", dest=\"TRAINBKGD\", default=False)\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea597406",
   "metadata": {},
   "source": [
    "### Preprocessing signal samples from all the decaymodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a341cf9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958bc2f0edab4270944c9e0ab3abf73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Read 1000000 events\n",
      "SCALE!!\n"
     ]
    }
   ],
   "source": [
    "events={'nn_rhorho':'RhoRhoEvent', 'nn_a1rho':'A1RhoEvent', 'nn_a1a1':'A1A1Event'}\n",
    "if args.REPRO:\n",
    "    for decaymode in tqdm(decaymodes):\n",
    "        points = []\n",
    "        args.IN = '/home/anderson/higgs_data/'+decaymode\n",
    "        args.TYPE = 'nn_'+decaymode\n",
    "        data, weights, argmaxs, perm, c012s, hits_argmaxs, hits_c012s = preprocess_data(args)\n",
    "        event = eval(events[args.TYPE])(data, args)\n",
    "        points.append(EventDatasets(event, weights, argmaxs, perm, c012s=c012s, hits_argmaxs=hits_argmaxs,  hits_c012s=hits_c012s, miniset=args.MINISET, unweighted=args.UNWEIGHTED))\n",
    "        pickle.dump(points,open(args.IN+'/events_wo_background.pk','wb'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "006cd6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# position={'nn_rhorho':[0,1,4,5,6,7], 'nn_a1rho':[0,1,2,3,5,6,7], 'nn_a1a1':[0,1,2,3,5,6,8,9]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cee035",
   "metadata": {},
   "source": [
    "### Loading signal samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6a38fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "points=pickle.load(open(args.IN+'/events_wo_background.pk','rb'))\n",
    "#points=pickle.load(open(args.IN+'/events_w_background.pk','rb')) # training with signal & background "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e548b8b",
   "metadata": {},
   "source": [
    "### Training NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47078747",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39972924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, rhorho_data_mc,rhorho_data_true,rhorho_labels_mc,rhorho_labels_true):\n",
    "        self.rhorho_data_mc = torch.from_numpy(rhorho_data_mc).float().to(device)\n",
    "        self.rhorho_data_true = torch.from_numpy(rhorho_data_true).float().to(device)\n",
    "        \n",
    "        self.rhorho_labels_mc =torch.from_numpy(rhorho_labels_mc).float().to(device)\n",
    "        self.rhorho_labels_true =torch.from_numpy(rhorho_labels_true).float().to(device)\n",
    "    def __getitem__(self, index):\n",
    "        return self.rhorho_data_mc[index],self.rhorho_data_true[index],self.rhorho_labels_mc[index],self.rhorho_labels_true[index]\n",
    "    def __len__(self):\n",
    "        return min(len(self.rhorho_labels_mc),len(self.rhorho_labels_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2404402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, num_layers=1, size=100, lr=1e-3, drop_prob=0, inplace=False, \n",
    "                 tloss=\"regr_weights\", activation='linear', input_noise=0.0, optimizer=\"AdamOptimizer\"):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_features,size,bias=False)\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.extend([nn.Linear(size,size,bias=False),\n",
    "                           nn.BatchNorm1d(size),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(drop_prob, inplace)\n",
    "                          ])\n",
    "        self.linear_relu_stack = nn.Sequential(*layers)\n",
    "        self.linear2 = nn.Linear(size,num_classes,bias=False)\n",
    "        self.linear3 = nn.Linear(size,2,bias=False)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        out = self.linear2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dafcb9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_idx=0\n",
    "\n",
    "mc_train_idx=np.random.choice(np.arange(points[particle_idx].train.x.shape[0]),int(points[particle_idx].train.x.shape[0]*0.5),replace=False)\n",
    "true_train_idx=list(set(np.arange(points[particle_idx].train.x.shape[0]))-set(mc_train_idx))\n",
    "\n",
    "mc_valid_idx=np.random.choice(np.arange(points[particle_idx].valid.x.shape[0]),int(points[particle_idx].valid.x.shape[0]*0.5),replace=False)\n",
    "true_valid_idx=list(set(np.arange(points[particle_idx].valid.x.shape[0]))-set(mc_train_idx))\n",
    "\n",
    "mc_test_idx=np.random.choice(np.arange(points[particle_idx].test.x.shape[0]),int(points[particle_idx].test.x.shape[0]*0.5),replace=False)\n",
    "true_test_idx=list(set(np.arange(points[particle_idx].test.x.shape[0]))-set(mc_train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5037f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty=0.0\n",
    "\n",
    "train_datasets = MyDataset(points[particle_idx].train.x[mc_train_idx], points[particle_idx].train.x[true_train_idx]+uncertainty*np.random.normal(0,1,size=points[particle_idx].train.x[true_train_idx].shape),\n",
    "                          points[particle_idx].train.weights[mc_train_idx],points[particle_idx].train.weights[true_train_idx])\n",
    "train_loader = DataLoader(dataset = train_datasets,batch_size = batch_size,shuffle = True)\n",
    "\n",
    "\n",
    "valid_datasets = MyDataset(points[particle_idx].valid.x[mc_valid_idx], points[particle_idx].valid.x[true_valid_idx]+uncertainty*np.random.normal(0,1,size=points[particle_idx].valid.x[true_valid_idx].shape),\n",
    "                          points[particle_idx].valid.weights[mc_valid_idx],points[particle_idx].valid.weights[true_valid_idx])\n",
    "valid_loader = DataLoader(dataset = valid_datasets,batch_size = batch_size,shuffle = True)\n",
    "\n",
    "\n",
    "test_datasets = MyDataset(points[particle_idx].test.x[mc_test_idx], points[particle_idx].test.x[true_test_idx]+uncertainty*np.random.normal(0,1,size=points[particle_idx].test.x[true_test_idx].shape),\n",
    "                          points[particle_idx].test.weights[mc_test_idx],points[particle_idx].test.weights[true_test_idx])\n",
    "test_loader = DataLoader(dataset = test_datasets,batch_size = batch_size,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c7064d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(num_features=points[particle_idx].train.x.shape[1], num_classes=args.NUM_CLASSES,num_layers=args.LAYERS,drop_prob=0).to(device)\n",
    "opt_g = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True,path='model/best_model.pt')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8691c833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " training loss: 2.712 \t acc: 0.153 \t\n",
      "\n",
      "  acc: 0.264 \t \n",
      "Validation loss decreased (inf --> -0.263702).  Saving model ...\n",
      " training loss: 2.046 \t acc: 0.276 \t\n",
      "\n",
      "  acc: 0.326 \t \n",
      "Validation loss decreased (-0.263702 --> -0.325602).  Saving model ...\n",
      " training loss: 1.874 \t acc: 0.314 \t\n",
      "\n",
      "  acc: 0.368 \t \n",
      "Validation loss decreased (-0.325602 --> -0.367607).  Saving model ...\n",
      " training loss: 1.776 \t acc: 0.338 \t\n",
      "\n",
      "  acc: 0.398 \t \n",
      "Validation loss decreased (-0.367607 --> -0.397632).  Saving model ...\n",
      " training loss: 1.706 \t acc: 0.356 \t\n",
      "\n",
      "  acc: 0.415 \t \n",
      "Validation loss decreased (-0.397632 --> -0.415318).  Saving model ...\n",
      " training loss: 1.656 \t acc: 0.367 \t\n",
      "\n",
      "  acc: 0.445 \t \n",
      "Validation loss decreased (-0.415318 --> -0.444701).  Saving model ...\n",
      " training loss: 1.615 \t acc: 0.378 \t\n",
      "\n",
      "  acc: 0.443 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.581 \t acc: 0.388 \t\n",
      "\n",
      "  acc: 0.464 \t \n",
      "Validation loss decreased (-0.444701 --> -0.464437).  Saving model ...\n",
      " training loss: 1.559 \t acc: 0.393 \t\n",
      "\n",
      "  acc: 0.475 \t \n",
      "Validation loss decreased (-0.464437 --> -0.474546).  Saving model ...\n",
      " training loss: 1.537 \t acc: 0.398 \t\n",
      "\n",
      "  acc: 0.484 \t \n",
      "Validation loss decreased (-0.474546 --> -0.484274).  Saving model ...\n",
      " training loss: 1.521 \t acc: 0.405 \t\n",
      "\n",
      "  acc: 0.491 \t \n",
      "Validation loss decreased (-0.484274 --> -0.490926).  Saving model ...\n",
      " training loss: 1.506 \t acc: 0.408 \t\n",
      "\n",
      "  acc: 0.506 \t \n",
      "Validation loss decreased (-0.490926 --> -0.505517).  Saving model ...\n",
      " training loss: 1.490 \t acc: 0.414 \t\n",
      "\n",
      "  acc: 0.487 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.477 \t acc: 0.417 \t\n",
      "\n",
      "  acc: 0.503 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.471 \t acc: 0.418 \t\n",
      "\n",
      "  acc: 0.523 \t \n",
      "Validation loss decreased (-0.505517 --> -0.523424).  Saving model ...\n",
      " training loss: 1.461 \t acc: 0.421 \t\n",
      "\n",
      "  acc: 0.527 \t \n",
      "Validation loss decreased (-0.523424 --> -0.527242).  Saving model ...\n",
      " training loss: 1.446 \t acc: 0.428 \t\n",
      "\n",
      "  acc: 0.509 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.434 \t acc: 0.431 \t\n",
      "\n",
      "  acc: 0.536 \t \n",
      "Validation loss decreased (-0.527242 --> -0.535985).  Saving model ...\n",
      " training loss: 1.432 \t acc: 0.431 \t\n",
      "\n",
      "  acc: 0.514 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.424 \t acc: 0.433 \t\n",
      "\n",
      "  acc: 0.536 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.415 \t acc: 0.438 \t\n",
      "\n",
      "  acc: 0.534 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.406 \t acc: 0.440 \t\n",
      "\n",
      "  acc: 0.535 \t \n",
      "EarlyStopping counter: 4 out of 10\n",
      " training loss: 1.401 \t acc: 0.441 \t\n",
      "\n",
      "  acc: 0.544 \t \n",
      "Validation loss decreased (-0.535985 --> -0.543522).  Saving model ...\n",
      " training loss: 1.398 \t acc: 0.443 \t\n",
      "\n",
      "  acc: 0.548 \t \n",
      "Validation loss decreased (-0.543522 --> -0.547822).  Saving model ...\n",
      " training loss: 1.387 \t acc: 0.446 \t\n",
      "\n",
      "  acc: 0.543 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.379 \t acc: 0.450 \t\n",
      "\n",
      "  acc: 0.545 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.378 \t acc: 0.450 \t\n",
      "\n",
      "  acc: 0.547 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.367 \t acc: 0.454 \t\n",
      "\n",
      "  acc: 0.537 \t \n",
      "EarlyStopping counter: 4 out of 10\n",
      " training loss: 1.363 \t acc: 0.454 \t\n",
      "\n",
      "  acc: 0.558 \t \n",
      "Validation loss decreased (-0.547822 --> -0.557791).  Saving model ...\n",
      " training loss: 1.361 \t acc: 0.455 \t\n",
      "\n",
      "  acc: 0.543 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.354 \t acc: 0.458 \t\n",
      "\n",
      "  acc: 0.555 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.350 \t acc: 0.460 \t\n",
      "\n",
      "  acc: 0.555 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.352 \t acc: 0.459 \t\n",
      "\n",
      "  acc: 0.567 \t \n",
      "Validation loss decreased (-0.557791 --> -0.566694).  Saving model ...\n",
      " training loss: 1.348 \t acc: 0.459 \t\n",
      "\n",
      "  acc: 0.553 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.339 \t acc: 0.463 \t\n",
      "\n",
      "  acc: 0.566 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.339 \t acc: 0.462 \t\n",
      "\n",
      "  acc: 0.569 \t \n",
      "Validation loss decreased (-0.566694 --> -0.569307).  Saving model ...\n",
      " training loss: 1.336 \t acc: 0.464 \t\n",
      "\n",
      "  acc: 0.551 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.327 \t acc: 0.467 \t\n",
      "\n",
      "  acc: 0.567 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.330 \t acc: 0.466 \t\n",
      "\n",
      "  acc: 0.572 \t \n",
      "Validation loss decreased (-0.569307 --> -0.571899).  Saving model ...\n",
      " training loss: 1.328 \t acc: 0.467 \t\n",
      "\n",
      "  acc: 0.561 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.323 \t acc: 0.467 \t\n",
      "\n",
      "  acc: 0.568 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.319 \t acc: 0.470 \t\n",
      "\n",
      "  acc: 0.561 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.313 \t acc: 0.471 \t\n",
      "\n",
      "  acc: 0.571 \t \n",
      "EarlyStopping counter: 4 out of 10\n",
      " training loss: 1.315 \t acc: 0.470 \t\n",
      "\n",
      "  acc: 0.563 \t \n",
      "EarlyStopping counter: 5 out of 10\n",
      " training loss: 1.311 \t acc: 0.473 \t\n",
      "\n",
      "  acc: 0.560 \t \n",
      "EarlyStopping counter: 6 out of 10\n",
      " training loss: 1.305 \t acc: 0.475 \t\n",
      "\n",
      "  acc: 0.581 \t \n",
      "Validation loss decreased (-0.571899 --> -0.580501).  Saving model ...\n",
      " training loss: 1.305 \t acc: 0.474 \t\n",
      "\n",
      "  acc: 0.580 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.306 \t acc: 0.474 \t\n",
      "\n",
      "  acc: 0.560 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.294 \t acc: 0.478 \t\n",
      "\n",
      "  acc: 0.577 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.299 \t acc: 0.477 \t\n",
      "\n",
      "  acc: 0.573 \t \n",
      "EarlyStopping counter: 4 out of 10\n",
      " training loss: 1.297 \t acc: 0.477 \t\n",
      "\n",
      "  acc: 0.575 \t \n",
      "EarlyStopping counter: 5 out of 10\n",
      " training loss: 1.287 \t acc: 0.480 \t\n",
      "\n",
      "  acc: 0.573 \t \n",
      "EarlyStopping counter: 6 out of 10\n",
      " training loss: 1.291 \t acc: 0.480 \t\n",
      "\n",
      "  acc: 0.573 \t \n",
      "EarlyStopping counter: 7 out of 10\n",
      " training loss: 1.289 \t acc: 0.480 \t\n",
      "\n",
      "  acc: 0.571 \t \n",
      "EarlyStopping counter: 8 out of 10\n",
      " training loss: 1.286 \t acc: 0.481 \t\n",
      "\n",
      "  acc: 0.573 \t \n",
      "EarlyStopping counter: 9 out of 10\n",
      " training loss: 1.285 \t acc: 0.481 \t\n",
      "\n",
      "  acc: 0.588 \t \n",
      "Validation loss decreased (-0.580501 --> -0.587857).  Saving model ...\n",
      " training loss: 1.285 \t acc: 0.482 \t\n",
      "\n",
      "  acc: 0.585 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.275 \t acc: 0.485 \t\n",
      "\n",
      "  acc: 0.574 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.276 \t acc: 0.484 \t\n",
      "\n",
      "  acc: 0.585 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.274 \t acc: 0.484 \t\n",
      "\n",
      "  acc: 0.587 \t \n",
      "EarlyStopping counter: 4 out of 10\n",
      " training loss: 1.274 \t acc: 0.486 \t\n",
      "\n",
      "  acc: 0.581 \t \n",
      "EarlyStopping counter: 5 out of 10\n",
      " training loss: 1.267 \t acc: 0.488 \t\n",
      "\n",
      "  acc: 0.595 \t \n",
      "Validation loss decreased (-0.587857 --> -0.594750).  Saving model ...\n",
      " training loss: 1.259 \t acc: 0.491 \t\n",
      "\n",
      "  acc: 0.589 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.265 \t acc: 0.489 \t\n",
      "\n",
      "  acc: 0.592 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.260 \t acc: 0.490 \t\n",
      "\n",
      "  acc: 0.589 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.259 \t acc: 0.491 \t\n",
      "\n",
      "  acc: 0.591 \t \n",
      "EarlyStopping counter: 4 out of 10\n",
      " training loss: 1.263 \t acc: 0.490 \t\n",
      "\n",
      "  acc: 0.588 \t \n",
      "EarlyStopping counter: 5 out of 10\n",
      " training loss: 1.257 \t acc: 0.491 \t\n",
      "\n",
      "  acc: 0.595 \t \n",
      "EarlyStopping counter: 6 out of 10\n",
      " training loss: 1.254 \t acc: 0.493 \t\n",
      "\n",
      "  acc: 0.585 \t \n",
      "EarlyStopping counter: 7 out of 10\n",
      " training loss: 1.254 \t acc: 0.492 \t\n",
      "\n",
      "  acc: 0.598 \t \n",
      "Validation loss decreased (-0.594750 --> -0.597866).  Saving model ...\n",
      " training loss: 1.256 \t acc: 0.492 \t\n",
      "\n",
      "  acc: 0.579 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.249 \t acc: 0.494 \t\n",
      "\n",
      "  acc: 0.591 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.254 \t acc: 0.494 \t\n",
      "\n",
      "  acc: 0.601 \t \n",
      "Validation loss decreased (-0.597866 --> -0.601383).  Saving model ...\n",
      " training loss: 1.247 \t acc: 0.496 \t\n",
      "\n",
      "  acc: 0.592 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.248 \t acc: 0.495 \t\n",
      "\n",
      "  acc: 0.601 \t \n",
      "Validation loss decreased (-0.601383 --> -0.601383).  Saving model ...\n",
      " training loss: 1.246 \t acc: 0.495 \t\n",
      "\n",
      "  acc: 0.595 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.234 \t acc: 0.501 \t\n",
      "\n",
      "  acc: 0.592 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.243 \t acc: 0.496 \t\n",
      "\n",
      "  acc: 0.599 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.242 \t acc: 0.497 \t\n",
      "\n",
      "  acc: 0.593 \t \n",
      "EarlyStopping counter: 4 out of 10\n",
      " training loss: 1.240 \t acc: 0.497 \t\n",
      "\n",
      "  acc: 0.593 \t \n",
      "EarlyStopping counter: 5 out of 10\n",
      " training loss: 1.239 \t acc: 0.498 \t\n",
      "\n",
      "  acc: 0.590 \t \n",
      "EarlyStopping counter: 6 out of 10\n",
      " training loss: 1.235 \t acc: 0.499 \t\n",
      "\n",
      "  acc: 0.592 \t \n",
      "EarlyStopping counter: 7 out of 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " training loss: 1.236 \t acc: 0.499 \t\n",
      "\n",
      "  acc: 0.596 \t \n",
      "EarlyStopping counter: 8 out of 10\n",
      " training loss: 1.229 \t acc: 0.502 \t\n",
      "\n",
      "  acc: 0.602 \t \n",
      "Validation loss decreased (-0.601383 --> -0.602187).  Saving model ...\n",
      " training loss: 1.235 \t acc: 0.500 \t\n",
      "\n",
      "  acc: 0.597 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.232 \t acc: 0.501 \t\n",
      "\n",
      "  acc: 0.592 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.230 \t acc: 0.501 \t\n",
      "\n",
      "  acc: 0.597 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.222 \t acc: 0.504 \t\n",
      "\n",
      "  acc: 0.600 \t \n",
      "EarlyStopping counter: 4 out of 10\n",
      " training loss: 1.228 \t acc: 0.502 \t\n",
      "\n",
      "  acc: 0.599 \t \n",
      "EarlyStopping counter: 5 out of 10\n",
      " training loss: 1.220 \t acc: 0.506 \t\n",
      "\n",
      "  acc: 0.602 \t \n",
      "Validation loss decreased (-0.602187 --> -0.602488).  Saving model ...\n",
      " training loss: 1.221 \t acc: 0.505 \t\n",
      "\n",
      "  acc: 0.601 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.222 \t acc: 0.505 \t\n",
      "\n",
      "  acc: 0.601 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.222 \t acc: 0.505 \t\n",
      "\n",
      "  acc: 0.595 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.216 \t acc: 0.506 \t\n",
      "\n",
      "  acc: 0.600 \t \n",
      "EarlyStopping counter: 4 out of 10\n",
      " training loss: 1.220 \t acc: 0.506 \t\n",
      "\n",
      "  acc: 0.594 \t \n",
      "EarlyStopping counter: 5 out of 10\n",
      " training loss: 1.217 \t acc: 0.507 \t\n",
      "\n",
      "  acc: 0.609 \t \n",
      "Validation loss decreased (-0.602488 --> -0.608859).  Saving model ...\n",
      " training loss: 1.217 \t acc: 0.506 \t\n",
      "\n",
      "  acc: 0.590 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.211 \t acc: 0.508 \t\n",
      "\n",
      "  acc: 0.595 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.217 \t acc: 0.506 \t\n",
      "\n",
      "  acc: 0.604 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.214 \t acc: 0.507 \t\n",
      "\n",
      "  acc: 0.596 \t \n",
      "EarlyStopping counter: 4 out of 10\n",
      " training loss: 1.210 \t acc: 0.509 \t\n",
      "\n",
      "  acc: 0.603 \t \n",
      "EarlyStopping counter: 5 out of 10\n",
      " training loss: 1.205 \t acc: 0.510 \t\n",
      "\n",
      "  acc: 0.608 \t \n",
      "EarlyStopping counter: 6 out of 10\n",
      " training loss: 1.208 \t acc: 0.509 \t\n",
      "\n",
      "  acc: 0.600 \t \n",
      "EarlyStopping counter: 7 out of 10\n",
      " training loss: 1.211 \t acc: 0.508 \t\n",
      "\n",
      "  acc: 0.607 \t \n",
      "EarlyStopping counter: 8 out of 10\n",
      " training loss: 1.211 \t acc: 0.507 \t\n",
      "\n",
      "  acc: 0.600 \t \n",
      "EarlyStopping counter: 9 out of 10\n",
      " training loss: 1.210 \t acc: 0.509 \t\n",
      "\n",
      "  acc: 0.603 \t \n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "epoch=200\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    train_loss,sample_numbers,acc,total_samples,bg_acc=0,0,0,0,0\n",
    "    for batch_idx, (rhorho_s,rhorho_t,label_s,_) in enumerate(train_loader):\n",
    "        \n",
    "        opt_g.zero_grad()\n",
    "        rhorho_s=rhorho_s[label_s.sum(axis=1)!=0]\n",
    "        label_s=label_s[label_s.sum(axis=1)!=0]\n",
    "        outputs=model(rhorho_s)\n",
    "        if isinstance(criterion,nn.CrossEntropyLoss):\n",
    "            loss=criterion(outputs,torch.argmax(label_s,axis=1))\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            acc+=(predictions==torch.argmax(label_s,axis=1)).sum().item()\n",
    "        else:\n",
    "            loss=criterion(outputs,label_s)\n",
    "        loss.backward()\n",
    "        train_loss+=loss.item()*len(rhorho_s)\n",
    "        sample_numbers+=len(rhorho_s)\n",
    "        opt_g.step()\n",
    "        print('\\r training loss: %.3f \\t acc: %.3f \\t' %(train_loss/sample_numbers,acc/sample_numbers),end='')\n",
    "    print()\n",
    "    vaild_acc,vaild_numbers,total_samples,bg_acc=0,0,0,0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (rhorho_s,rhorho_t,label_s,label_t) in enumerate(valid_loader):\n",
    "            total_samples+=len(rhorho_t)\n",
    "            rhorho_t= rhorho_t[label_t.sum(axis=1)!=0]\n",
    "            label_t = label_t[label_t.sum(axis=1)!=0]\n",
    "            outputs=model(rhorho_t)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            vaild_acc+=(predictions==torch.argmax(label_t,axis=1)).sum().item()\n",
    "            vaild_numbers+=len(rhorho_t)\n",
    "    print()\n",
    "    print('\\r  acc: %.3f \\t ' %(vaild_acc/vaild_numbers),end='')\n",
    "    print()\n",
    "    early_stopping(-vaild_acc/vaild_numbers,model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break;\n",
    "# test_loss=0\n",
    "# with torch.no_grad():\n",
    "#     for inputs, label in test_loader:\n",
    "#         outputs=model(inputs)\n",
    "#         test_loss+=mse_loss(outputs,label).item()*len(inputs)\n",
    "#     print('test loss: %f' %(test_loss/len(test_loader.dataset.tensors[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8eb181",
   "metadata": {},
   "source": [
    "### Converting bkgd raw data into npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a605ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_bkgd_raw(args.BKGDPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1555fd9",
   "metadata": {},
   "source": [
    "### Preprocessing singal and bkgd from all the decaymodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f30480ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1b631dea8942429f09e7ecf623d11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Read 1000000 events\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5501/1587744381.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'nn_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdecaymode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAINBKGD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margmaxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc012s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhits_argmaxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhits_c012s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEventDatasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margmaxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc012s\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc012s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhits_argmaxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhits_argmaxs\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mhits_c012s\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhits_c012s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminiset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMINISET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munweighted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNWEIGHTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HiggsCP/src_py/cpmix_utils.py\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margmaxs\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mhits_argmaxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_weights_and_argmaxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc012s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;31m# trying with binary classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAINBKGD\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HiggsCP/src_py/cpmix_utils.py\u001b[0m in \u001b[0;36mcalc_weights_and_argmaxs\u001b[0;34m(classes, c012s, data_len, num_classes)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0margmaxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mhits_argmaxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhits_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margmaxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhits_argmaxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/HiggsCP/src_py/cpmix_utils.py\u001b[0m in \u001b[0;36mhits_fun\u001b[0;34m(classes, x, num_classes)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mhits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m  \u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m              \u001b[0mhits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m            \u001b[0;31m# 落在class之间的 x 算在左边的class里，左边的hit等于1， 相当于给x分类\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# events={'nn_rhorho':'RhoRhoEvent', 'nn_a1rho':'A1RhoEvent', 'nn_a1a1':'A1A1Event'}\n",
    "if args.REPRO:\n",
    "    for decaymode in tqdm(decaymodes):\n",
    "        points = []\n",
    "        args.Z_NOISE_FRACTION = 1\n",
    "        args.IN = '/home/anderson/higgs_data/'+decaymode\n",
    "        args.TYPE = 'nn_'+decaymode\n",
    "        data, weights, argmaxs, perm, c012s, hits_argmaxs, hits_c012s = preprocess_data(args)\n",
    "        event = eval(events[args.TYPE])(data, args)\n",
    "        points.append(EventDatasets(event, weights, argmaxs, perm, c012s=c012s, hits_argmaxs=hits_argmaxs,  hits_c012s=hits_c012s, miniset=args.MINISET, unweighted=args.UNWEIGHTED))\n",
    "        pickle.dump(points,open(args.IN+'/events_w_background.pk','wb'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d05c89",
   "metadata": {},
   "source": [
    "### Loading bkgd samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f5aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_points=pickle.load(open(args.IN+'/events_w_background.pk','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea18550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_idx = 0 # 0 for rhorho\n",
    "background=[]\n",
    "background.append(background_points[particle_idx].train.x[background_points[particle_idx].train.weights.sum(axis=1)==0])\n",
    "background.append(background_points[particle_idx].valid.x[background_points[particle_idx].valid.weights.sum(axis=1)==0])\n",
    "background.append(background_points[particle_idx].test.x[background_points[particle_idx].test.weights.sum(axis=1)==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "background=np.concatenate(background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99964c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "background=torch.tensor(background).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3885c6",
   "metadata": {},
   "source": [
    "### Testing NN w/ bkgd only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5c4a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model/best_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs=[]\n",
    "    for i in tqdm(range(0, 400000,batch_size)):\n",
    "        outputs.append(model(background[i:i+batch_size]).detach().cpu())\n",
    "outputs=torch.cat(outputs)\n",
    "\n",
    "bg_outputs=torch.argmax(torch.softmax(outputs,axis=1),axis=1).numpy()\n",
    "bg_labels_counts=np.unique(bg_outputs,return_counts=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bg_outputs,open(args.IN+'/NN_outputs_background_only.pk','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5464f5",
   "metadata": {},
   "source": [
    "### Testing NN w/ signal only (Class 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c45659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model/best_model.pt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    signal_outputs,signal_labels=[],[]\n",
    "    for batch_idx, (rhorho_s,rhorho_t,label_s,_) in enumerate(train_loader):\n",
    "        signal_outputs.append(model(rhorho_s).detach().cpu())\n",
    "        signal_labels.append(label_s.detach().cpu())\n",
    "signal_outputs=torch.softmax(torch.cat(signal_outputs),axis=1).numpy()\n",
    "signal_labels=np.concatenate(signal_labels)\n",
    "\n",
    "####### Filtering signal outputs that are classified to Class 0\n",
    "signal_outputs=signal_outputs[np.argmax(signal_labels,axis=1)==5]\n",
    "signal_labels=np.argmax(signal_outputs,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e367615",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(signal_outputs,open(args.IN+'/NN_outputs_signal_only.pk','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a5a8e",
   "metadata": {},
   "source": [
    "### Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d400b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_outputs = pickle.load(open(args.IN+'/NN_outputs_background_only.pk','rb'))\n",
    "signal_outputs= pickle.load(open(args.IN+'/NN_outputs_signal_only.pk','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a5e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.fill_between(np.arange(args.NUM_CLASSES-1),0,bg_labels_counts,alpha=1,hatch='/', facecolor=\"skyblue\")\n",
    "#plt.fill_between(np.arange(args.NUM_CLASSES-1),bg_labels_counts,bg_labels_counts+signal_labels_counts,color='red',alpha=1)\n",
    "#plt.fill_between(np.arange(args.NUM_CLASSES-1),outputs.mean(axis=0)[:args.NUM_CLASSES-1],outputs.mean(axis=0)[:args.NUM_CLASSES-1]+signal_outputs[np.argmax(signal_labels,axis=1)==0].mean(axis=0)[:args.NUM_CLASSES-1],color='red',alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb9962",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create the dataframe; enumerate is used to make column names\n",
    "#columns=['Ztt','Signal']\n",
    "#fig,ax=plt.subplots(dpi=150)\n",
    "#plt.xlabel(\"Classes\")\n",
    "#lt.ylabel(\"Events\")\n",
    "#df = pd.concat([pd.DataFrame(a, columns=[columns[i]]) for i, a in enumerate([bg_outputs, np.argmax(signal_outputs,axis=1)], 0)], axis=1)\n",
    "\n",
    "# plot the data\n",
    "#ax.set_xlim(0,args.NUM_CLASSES-2)\n",
    "#ax = df.plot.hist(stacked=True, bins=args.NUM_CLASSES-1,ax=ax, color = ['skyblue','red']).get_figure()\n",
    "# ax.set_xticks(np.arange(args.NUM_CLASSES-2))\n",
    "# ax.set_xticklabels((np.linspace(0,2,args.NUM_CLASSES-2)*np.pi))\n",
    "# bars = ax.patches\n",
    "# hatches = ['/','\\\\']\n",
    "\n",
    "# for i in range(2):\n",
    "#     for j in range(args.NUM_CLASSES-1):\n",
    "#         bars[i*(args.NUM_CLASSES-1)+j].set_hatch(hatches[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc973d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(dpi=150)\n",
    "\n",
    "Ztt = bg_outputs\n",
    "Signal = np.array(np.argmax(signal_outputs,axis=1))\n",
    "\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Events\")\n",
    "\n",
    "# plot the data\n",
    "bins = args.NUM_CLASSES-1\n",
    "ax.set_xlim(0,bins-1)\n",
    "#entries, edges, _ = ax.hist(Ztt, bins, color = 'skyblue', stacked=True)\n",
    "entries, edges, _ = plt.hist([Ztt,Signal], bins, color = ['skyblue', 'red'], stacked=True, label=['Ztt','Signal'])\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "# calculate bin centers\n",
    "bin_centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "\n",
    "# draw error bars\n",
    "Ztt_std = np.sqrt(entries[0])\n",
    "plt.errorbar(bin_centers, entries[0], yerr=Ztt_std, ls='none', ecolor='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5829338",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(dpi=150)\n",
    "\n",
    "Ztt = bg_outputs\n",
    "Signal = np.array(np.argmax(signal_outputs,axis=1))\n",
    "\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Events\")\n",
    "\n",
    "# plot the data\n",
    "bins = args.NUM_CLASSES-1\n",
    "ax.set_xlim(0,bins-1)\n",
    "#entries, edges, _ = ax.hist(Ztt, bins, color = 'skyblue', stacked=True)\n",
    "#entries, edges, _ = plt.hist(Ztt, bins, color = 'skyblue', label='Ztt')\n",
    "entries, edges, _ = plt.hist(Signal, bins, color = 'red', label='Signal')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "# calculate bin centers\n",
    "#bin_centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "\n",
    "# draw error bars\n",
    "#Ztt_std = np.sqrt(entries)\n",
    "#plt.errorbar(bin_centers, entries, yerr=Ztt_std, ls='none', ecolor='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1784e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2efb10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(dpi=150)\n",
    "\n",
    "Ztt = bg_outputs\n",
    "Signal = np.array(np.argmax(signal_outputs,axis=1))\n",
    "\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Events\")\n",
    "\n",
    "# plot the data\n",
    "bins = args.NUM_CLASSES-1\n",
    "ax.set_xlim(0,bins-1)\n",
    "#entries, edges, _ = ax.hist(Ztt, bins, color = 'skyblue', stacked=True)\n",
    "entries, edges, _ = plt.hist(Ztt, bins, color = 'skyblue', label='Ztt')\n",
    "#entries, edges, _ = plt.hist(Signal, bins, color = 'red', label='Signal')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "# calculate bin centers\n",
    "bin_centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "\n",
    "# draw error bars\n",
    "Ztt_std = np.sqrt(entries)\n",
    "plt.errorbar(bin_centers, entries, yerr=Ztt_std, ls='none', ecolor='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0bb3c2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1576.1605403285826\n"
     ]
    }
   ],
   "source": [
    "print(np.std(entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66b015c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11458. 15631. 11366. 16940. 10046. 14560. 12296. 13356. 12647. 14653.\n",
      " 11293. 14678. 12066. 13458. 13228. 12613. 14095. 12716. 14885. 10827.\n",
      " 15793. 11412. 15006. 13441. 13570. 13454. 12826. 14590. 13150. 14330.]\n"
     ]
    }
   ],
   "source": [
    "print(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1194f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [38803, 40284, 39509, 41222, 41121, 40127, 41078, 39621, 41184, 37435]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53514383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1174.421576777266\n"
     ]
    }
   ],
   "source": [
    "print(np.std(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd4a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
