{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87e49442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from Eearly_stop import *\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import sys\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from src_py.cpmix_utils import preprocess_data\n",
    "from src_py.rhorho import RhoRhoEvent\n",
    "from src_py.a1a1 import A1A1Event\n",
    "from src_py.a1rho import A1RhoEvent\n",
    "from src_py.data_utils import read_np, EventDatasets\n",
    "from src_py.process_background import convert_bkgd_raw\n",
    "import train_rhorho, train_a1rho, train_a1a1\n",
    "from src_py.metrics_utils import calculate_deltas_unsigned, calculate_deltas_signed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5e91a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da403d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decaymodes = ['rhorho', 'a1rho', 'a1a1']\n",
    "types = {\"nn_rhorho\": train_rhorho.start,\"nn_a1rho\": train_a1rho.start,\"nn_a1a1\": train_a1a1.start}\n",
    "parser = argparse.ArgumentParser(description='Train classifier')\n",
    "\n",
    "parser.add_argument(\"--num_classes\", dest=\"NUM_CLASSES\", type=int, default=11)\n",
    "parser.add_argument(\"-l\", \"--layers\", dest=\"LAYERS\", type=int, help = \"number of NN layers\", default=6)\n",
    "parser.add_argument(\"-s\", \"--size\", dest=\"SIZE\", type=int, help=\"NN size\", default=100)\n",
    "parser.add_argument(\"-lambda\", \"--lambda\", type=float, dest=\"LAMBDA\", help=\"value of lambda parameter\", default=0.0)\n",
    "parser.add_argument(\"-m\", \"--method\", dest=\"METHOD\", choices=[\"A\", \"B\", \"C\"], default=\"A\")\n",
    "parser.add_argument(\"-o\", \"--optimizer\", dest=\"OPT\", \n",
    "    choices=[\"GradientDescentOptimizer\", \"AdadeltaOptimizer\", \"AdagradOptimizer\",\n",
    "         \"ProximalAdagradOptimizer\", \"AdamOptimizer\", \"FtrlOptimizer\",\n",
    "         \"ProximalGradientDescentOptimizer\", \"RMSPropOptimizer\"], default=\"AdamOptimizer\")\n",
    "parser.add_argument(\"-d\", \"--dropout\", dest=\"DROPOUT\", type=float, default=0.0)\n",
    "parser.add_argument(\"-e\", \"--epochs\", dest=\"EPOCHS\", type=int, default=3)\n",
    "# parser.add_argument(\"-f\", \"--features\", dest=\"FEAT\", help=\"Features\", default=\"Variant-All\")\n",
    "# #         choices= [\"Variant-All\", \"Variant-1.0\", \"Variant-1.1\", \"Variant-2.0\", \"Variant-2.1\",\n",
    "# #                   \"Variant-2.2\", \"Variant-3.0\", \"Variant-3.1\", \"Variant-4.0\", \"Variant-4.1\"])\n",
    "\n",
    "########### Change this to according dir to download data #######################\n",
    "parser.add_argument(\"-i\", \"--input\", dest=\"IN\", default='/home/anderson/higgs_data/rhorho')\n",
    "\n",
    "parser.add_argument(\"--miniset\", dest=\"MINISET\", type=lambda s: s.lower() in ['true', 't', 'yes', '1'], default=False)\n",
    "parser.add_argument(\"--z_noise_fraction\", dest=\"Z_NOISE_FRACTION\", type=float, default=0.0)\n",
    "parser.add_argument(\"--z_fraction\", dest=\"Z_FRACTION\", type=float, default=0.0)\n",
    "\n",
    "parser.add_argument(\"--delt_classes\", dest=\"DELT_CLASSES\", type=int, default=0,\n",
    "                    help='Maximal distance between predicted and valid class for event being considered as correctly classified')\n",
    "\n",
    "parser.add_argument(\"--unweighted\", dest=\"UNWEIGHTED\", type=lambda s: s.lower() in ['true', 't', 'yes', '1'], default=False)\n",
    "parser.add_argument(\"--reuse_weights\", dest=\"REUSE_WEIGHTS\", type=bool, default=False)\n",
    "parser.add_argument(\"--restrict_most_probable_angle\", dest=\"RESTRICT_MOST_PROBABLE_ANGLE\", type=bool, default=False)\n",
    "parser.add_argument(\"--force_download\", dest=\"FORCE_DOWNLOAD\", type=bool, default=False)\n",
    "parser.add_argument(\"--normalize_weights\", dest=\"NORMALIZE_WEIGHTS\", type=bool, default=False)\n",
    "\n",
    "\n",
    "parser.add_argument(\"--beta\",  type=float, dest=\"BETA\", help=\"value of beta parameter for polynomial smearing\", default=0.0)\n",
    "parser.add_argument(\"--pol_b\", type=float, dest=\"pol_b\", help=\"value of b parameter for polynomial smearing\", default=0.0)\n",
    "parser.add_argument(\"--pol_c\", type=float, dest=\"pol_c\", help=\"value of c parameter for polynomial smearing\", default=0.0)\n",
    "\n",
    "parser.add_argument(\"--w1\", dest=\"W1\")\n",
    "parser.add_argument(\"--w2\", dest=\"W2\")\n",
    "parser.add_argument(\"--f\", dest=\"FEAT\", default=\"Variant-All\")\n",
    "parser.add_argument(\"--plot_features\", dest=\"PLOT_FEATURES\", choices=[\"NO\", \"FILTER\", \"NO-FILTER\"], default=\"NO\")\n",
    "parser.add_argument(\"--training_method\", dest=\"TRAINING_METHOD\", choices=[\"soft_weights\", \"soft_c012s\",  \"soft_argmaxs\", \"regr_c012s\", \"regr_weights\", \"regr_argmaxs\"], default=\"soft_weights\")\n",
    "parser.add_argument(\"--hits_c012s\", dest=\"HITS_C012s\", choices=[\"hits_c0s\", \"hits_c1s\",  \"hits_c2s\"], default=\"hits_c0s\")\n",
    "\n",
    "######Change this to according type (rhorho, a1rho, a1a1)#######################\n",
    "parser.add_argument(\"-t\", \"--type\", dest=\"TYPE\", choices=types.keys(), default='nn_rhorho')\n",
    "\n",
    "parser.add_argument(\"-r\", \"--reprocess\", dest=\"REPRO\", type=bool, default=False)\n",
    "parser.add_argument(\"-bkgd\", \"--bkgdpath\", dest=\"BKGDPATH\", default='/home/anderson/Ztt_raw/pythia.Z_115_135.%s.1M.*.outTUPLE_labFrame')\n",
    "parser.add_argument(\"--train_bkgd\", dest=\"TRAINBKGD\", default=False)\n",
    "parser.add_argument(\"--remove_feature\", dest=\"REMOVE_FEATURE\", default=0)\n",
    "\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea597406",
   "metadata": {},
   "source": [
    "### Preprocessing signal samples from all the decaymodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a341cf9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "events={'nn_rhorho':'RhoRhoEvent', 'nn_a1rho':'A1RhoEvent', 'nn_a1a1':'A1A1Event'}\n",
    "if args.REPRO:\n",
    "    for decaymode in tqdm(decaymodes):\n",
    "        points = []\n",
    "        args.IN = '/home/anderson/higgs_data/'+decaymode\n",
    "        args.TYPE = 'nn_'+decaymode\n",
    "        data, weights, argmaxs, perm, c012s, hits_argmaxs, hits_c012s = preprocess_data(args)\n",
    "        event = eval(events[args.TYPE])(data, args)\n",
    "        points.append(EventDatasets(event, weights, argmaxs, perm, c012s=c012s, hits_argmaxs=hits_argmaxs,  hits_c012s=hits_c012s, miniset=args.MINISET, unweighted=args.UNWEIGHTED))\n",
    "        pickle.dump(points,open(args.IN+'/events_w_background_train_Z_FRACTION_'+str(args.Z_FRACTION)+'.pk','wb'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "006cd6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# position={'nn_rhorho':[0,1,4,5,6,7], 'nn_a1rho':[0,1,2,3,5,6,7], 'nn_a1a1':[0,1,2,3,5,6,8,9]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cee035",
   "metadata": {},
   "source": [
    "### Loading signal samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6a38fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "points=pickle.load(open(args.IN+'/events_wo_background.pk','rb'))\n",
    "#points=pickle.load(open(args.IN+'/events_w_background.pk','rb')) # training with signal & background "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e548b8b",
   "metadata": {},
   "source": [
    "### Training NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47078747",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39972924",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, rhorho_data_mc,rhorho_data_true,rhorho_labels_mc,rhorho_labels_true):\n",
    "        self.rhorho_data_mc = torch.from_numpy(rhorho_data_mc).float().to(device)\n",
    "        self.rhorho_data_true = torch.from_numpy(rhorho_data_true).float().to(device)\n",
    "        \n",
    "        self.rhorho_labels_mc = torch.from_numpy(rhorho_labels_mc).float().to(device)\n",
    "        self.rhorho_labels_true = torch.from_numpy(rhorho_labels_true).float().to(device)\n",
    "    def __getitem__(self, index):\n",
    "        return self.rhorho_data_mc[index],self.rhorho_data_true[index],self.rhorho_labels_mc[index],self.rhorho_labels_true[index]\n",
    "    def __len__(self):\n",
    "        return min(len(self.rhorho_labels_mc),len(self.rhorho_labels_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2404402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, num_layers=1, size=100, lr=1e-3, drop_prob=0, inplace=False, \n",
    "                 tloss=\"regr_weights\", activation='linear', input_noise=0.0, optimizer=\"AdamOptimizer\"):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_features,size,bias=False)\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.extend([nn.Linear(size,size,bias=False),\n",
    "                           nn.BatchNorm1d(size),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Dropout(drop_prob, inplace)\n",
    "                          ])\n",
    "        self.linear_relu_stack = nn.Sequential(*layers)\n",
    "        self.linear2 = nn.Linear(size,num_classes,bias=False)\n",
    "        self.linear3 = nn.Linear(size,2,bias=False)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        out = self.linear2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dafcb9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_idx=0\n",
    "\n",
    "mc_train_idx = np.random.choice(np.arange(points[particle_idx].train.x.shape[0]), int(points[particle_idx].train.x.shape[0]*0.5), replace=False)\n",
    "true_train_idx = list(set(np.arange(points[particle_idx].train.x.shape[0]))-set(mc_train_idx))\n",
    "\n",
    "mc_valid_idx = np.random.choice(np.arange(points[particle_idx].valid.x.shape[0]), int(points[particle_idx].valid.x.shape[0]*0.5), replace=False)\n",
    "true_valid_idx = list(set(np.arange(points[particle_idx].valid.x.shape[0]))-set(mc_train_idx))\n",
    "\n",
    "mc_test_idx = np.random.choice(np.arange(points[particle_idx].test.x.shape[0]), int(points[particle_idx].test.x.shape[0]*0.5), replace=False)\n",
    "true_test_idx = list(set(np.arange(points[particle_idx].test.x.shape[0])) - set(mc_train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e0c09d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_feature = args.REMOVE_FEATURE\n",
    "# removed_feature = 0\n",
    "\n",
    "new_train_x = np.delete(points[particle_idx].train.x, removed_feature, 1)\n",
    "new_valid_x = np.delete(points[particle_idx].valid.x, removed_feature, 1)\n",
    "new_test_x = np.delete(points[particle_idx].test.x, removed_feature, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5037f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty=0.0\n",
    "\n",
    "train_datasets = MyDataset(\n",
    "                            new_train_x[mc_train_idx], \n",
    "                            new_train_x[true_train_idx] + uncertainty*np.random.normal(0,1,size=new_train_x[true_train_idx].shape),\n",
    "                            points[particle_idx].train.weights[mc_train_idx],\n",
    "                            points[particle_idx].train.weights[true_train_idx]\n",
    "                          )\n",
    "train_loader = DataLoader(dataset = train_datasets, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "valid_datasets = MyDataset(\n",
    "                            new_valid_x[mc_valid_idx], \n",
    "                            new_valid_x[true_valid_idx] + uncertainty*np.random.normal(0,1,size=new_valid_x[true_valid_idx].shape),\n",
    "                            points[particle_idx].valid.weights[mc_valid_idx],\n",
    "                            points[particle_idx].valid.weights[true_valid_idx]\n",
    "                          )\n",
    "valid_loader = DataLoader(dataset = valid_datasets,batch_size = batch_size,shuffle = True)\n",
    "\n",
    "\n",
    "test_datasets = MyDataset(\n",
    "                            new_test_x[mc_test_idx], \n",
    "                            new_test_x[true_test_idx] + uncertainty*np.random.normal(0,1,size=new_test_x[true_test_idx].shape),\n",
    "                            points[particle_idx].test.weights[mc_test_idx],\n",
    "                            points[particle_idx].test.weights[true_test_idx]\n",
    "                         )\n",
    "test_loader = DataLoader(dataset = test_datasets,batch_size = batch_size,shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c7064d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(num_features=new_train_x.shape[1], num_classes=args.NUM_CLASSES,num_layers=args.LAYERS,drop_prob=0).to(device)\n",
    "opt_g = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "save_model_path = 'model/%d_model_%d.pt'%(args.NUM_CLASSES, args.REMOVE_FEATURE)\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True, path=save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8691c833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " training loss: 1.803 \t acc: 0.336 \t\n",
      "\n",
      "  acc: 0.423 \t \n",
      "Validation loss decreased (inf --> -0.423389).  Saving model ...\n",
      " training loss: 1.520 \t acc: 0.433 \t\n",
      "\n",
      "  acc: 0.462 \t \n",
      "Validation loss decreased (-0.423389 --> -0.461609).  Saving model ...\n",
      " training loss: 1.434 \t acc: 0.459 \t\n",
      "\n",
      "  acc: 0.469 \t \n",
      "Validation loss decreased (-0.461609 --> -0.469454).  Saving model ...\n",
      " training loss: 1.381 \t acc: 0.474 \t\n",
      "\n",
      "  acc: 0.487 \t \n",
      "Validation loss decreased (-0.469454 --> -0.486588).  Saving model ...\n",
      " training loss: 1.343 \t acc: 0.485 \t\n",
      "\n",
      "  acc: 0.499 \t \n",
      "Validation loss decreased (-0.486588 --> -0.499027).  Saving model ...\n",
      " training loss: 1.314 \t acc: 0.492 \t\n",
      "\n",
      "  acc: 0.513 \t \n",
      "Validation loss decreased (-0.499027 --> -0.513292).  Saving model ...\n",
      " training loss: 1.283 \t acc: 0.503 \t\n",
      "\n",
      "  acc: 0.518 \t \n",
      "Validation loss decreased (-0.513292 --> -0.518147).  Saving model ...\n",
      " training loss: 1.263 \t acc: 0.510 \t\n",
      "\n",
      "  acc: 0.522 \t \n",
      "Validation loss decreased (-0.518147 --> -0.522059).  Saving model ...\n",
      " training loss: 1.245 \t acc: 0.516 \t\n",
      "\n",
      "  acc: 0.533 \t \n",
      "Validation loss decreased (-0.522059 --> -0.532632).  Saving model ...\n",
      " training loss: 1.230 \t acc: 0.519 \t\n",
      "\n",
      "  acc: 0.535 \t \n",
      "Validation loss decreased (-0.532632 --> -0.534518).  Saving model ...\n",
      " training loss: 1.217 \t acc: 0.524 \t\n",
      "\n",
      "  acc: 0.533 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.204 \t acc: 0.527 \t\n",
      "\n",
      "  acc: 0.536 \t \n",
      "Validation loss decreased (-0.534518 --> -0.535562).  Saving model ...\n",
      " training loss: 1.194 \t acc: 0.531 \t\n",
      "\n",
      "  acc: 0.540 \t \n",
      "Validation loss decreased (-0.535562 --> -0.540497).  Saving model ...\n",
      " training loss: 1.185 \t acc: 0.534 \t\n",
      "\n",
      "  acc: 0.545 \t \n",
      "Validation loss decreased (-0.540497 --> -0.544670).  Saving model ...\n",
      " training loss: 1.173 \t acc: 0.537 \t\n",
      "\n",
      "  acc: 0.547 \t \n",
      "Validation loss decreased (-0.544670 --> -0.547278).  Saving model ...\n",
      " training loss: 1.165 \t acc: 0.539 \t\n",
      "\n",
      "  acc: 0.552 \t \n",
      "Validation loss decreased (-0.547278 --> -0.552455).  Saving model ...\n",
      " training loss: 1.159 \t acc: 0.541 \t\n",
      "\n",
      "  acc: 0.556 \t \n",
      "Validation loss decreased (-0.552455 --> -0.556006).  Saving model ...\n",
      " training loss: 1.151 \t acc: 0.544 \t\n",
      "\n",
      "  acc: 0.560 \t \n",
      "Validation loss decreased (-0.556006 --> -0.560279).  Saving model ...\n",
      " training loss: 1.144 \t acc: 0.547 \t\n",
      "\n",
      "  acc: 0.554 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.139 \t acc: 0.548 \t\n",
      "\n",
      "  acc: 0.559 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.131 \t acc: 0.551 \t\n",
      "\n",
      "  acc: 0.560 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.128 \t acc: 0.552 \t\n",
      "\n",
      "  acc: 0.561 \t \n",
      "Validation loss decreased (-0.560279 --> -0.561022).  Saving model ...\n",
      " training loss: 1.120 \t acc: 0.555 \t\n",
      "\n",
      "  acc: 0.565 \t \n",
      "Validation loss decreased (-0.561022 --> -0.564874).  Saving model ...\n",
      " training loss: 1.117 \t acc: 0.555 \t\n",
      "\n",
      "  acc: 0.564 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.110 \t acc: 0.557 \t\n",
      "\n",
      "  acc: 0.567 \t \n",
      "Validation loss decreased (-0.564874 --> -0.567422).  Saving model ...\n",
      " training loss: 1.106 \t acc: 0.559 \t\n",
      "\n",
      "  acc: 0.570 \t \n",
      "Validation loss decreased (-0.567422 --> -0.569950).  Saving model ...\n",
      " training loss: 1.101 \t acc: 0.560 \t\n",
      "\n",
      "  acc: 0.566 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.099 \t acc: 0.562 \t\n",
      "\n",
      "  acc: 0.570 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.095 \t acc: 0.563 \t\n",
      "\n",
      "  acc: 0.571 \t \n",
      "Validation loss decreased (-0.569950 --> -0.570933).  Saving model ...\n",
      " training loss: 1.089 \t acc: 0.565 \t\n",
      "\n",
      "  acc: 0.577 \t \n",
      "Validation loss decreased (-0.570933 --> -0.577373).  Saving model ...\n",
      " training loss: 1.090 \t acc: 0.564 \t\n",
      "\n",
      "  acc: 0.574 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.084 \t acc: 0.566 \t\n",
      "\n",
      "  acc: 0.579 \t \n",
      "Validation loss decreased (-0.577373 --> -0.579038).  Saving model ...\n",
      " training loss: 1.080 \t acc: 0.568 \t\n",
      "\n",
      "  acc: 0.576 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.076 \t acc: 0.569 \t\n",
      "\n",
      "  acc: 0.578 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.073 \t acc: 0.570 \t\n",
      "\n",
      "  acc: 0.582 \t \n",
      "Validation loss decreased (-0.579038 --> -0.581546).  Saving model ...\n",
      " training loss: 1.070 \t acc: 0.571 \t\n",
      "\n",
      "  acc: 0.578 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.067 \t acc: 0.571 \t\n",
      "\n",
      "  acc: 0.578 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.064 \t acc: 0.573 \t\n",
      "\n",
      "  acc: 0.581 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.061 \t acc: 0.574 \t\n",
      "\n",
      "  acc: 0.583 \t \n",
      "Validation loss decreased (-0.581546 --> -0.583251).  Saving model ...\n",
      " training loss: 1.060 \t acc: 0.574 \t\n",
      "\n",
      "  acc: 0.578 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.057 \t acc: 0.576 \t\n",
      "\n",
      "  acc: 0.584 \t \n",
      "Validation loss decreased (-0.583251 --> -0.583653).  Saving model ...\n",
      " training loss: 1.054 \t acc: 0.576 \t\n",
      "\n",
      "  acc: 0.578 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.051 \t acc: 0.578 \t\n",
      "\n",
      "  acc: 0.581 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.050 \t acc: 0.578 \t\n",
      "\n",
      "  acc: 0.584 \t \n",
      "Validation loss decreased (-0.583653 --> -0.584074).  Saving model ...\n",
      " training loss: 1.049 \t acc: 0.578 \t\n",
      "\n",
      "  acc: 0.583 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.045 \t acc: 0.580 \t\n",
      "\n",
      "  acc: 0.581 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.041 \t acc: 0.581 \t\n",
      "\n",
      "  acc: 0.582 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.041 \t acc: 0.582 \t\n",
      "\n",
      "  acc: 0.580 \t \n",
      "EarlyStopping counter: 4 out of 10\n",
      " training loss: 1.039 \t acc: 0.582 \t\n",
      "\n",
      "  acc: 0.583 \t \n",
      "EarlyStopping counter: 5 out of 10\n",
      " training loss: 1.038 \t acc: 0.582 \t\n",
      "\n",
      "  acc: 0.587 \t \n",
      "Validation loss decreased (-0.584074 --> -0.586542).  Saving model ...\n",
      " training loss: 1.034 \t acc: 0.585 \t\n",
      "\n",
      "  acc: 0.584 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.033 \t acc: 0.585 \t\n",
      "\n",
      "  acc: 0.588 \t \n",
      "Validation loss decreased (-0.586542 --> -0.587585).  Saving model ...\n",
      " training loss: 1.032 \t acc: 0.584 \t\n",
      "\n",
      "  acc: 0.587 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.029 \t acc: 0.585 \t\n",
      "\n",
      "  acc: 0.585 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.025 \t acc: 0.587 \t\n",
      "\n",
      "  acc: 0.586 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.028 \t acc: 0.585 \t\n",
      "\n",
      "  acc: 0.586 \t \n",
      "EarlyStopping counter: 4 out of 10\n",
      " training loss: 1.027 \t acc: 0.585 \t\n",
      "\n",
      "  acc: 0.591 \t \n",
      "Validation loss decreased (-0.587585 --> -0.590635).  Saving model ...\n",
      " training loss: 1.022 \t acc: 0.588 \t\n",
      "\n",
      "  acc: 0.587 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.024 \t acc: 0.587 \t\n",
      "\n",
      "  acc: 0.585 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.022 \t acc: 0.588 \t\n",
      "\n",
      "  acc: 0.590 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.020 \t acc: 0.589 \t\n",
      "\n",
      "  acc: 0.595 \t \n",
      "Validation loss decreased (-0.590635 --> -0.594948).  Saving model ...\n",
      " training loss: 1.018 \t acc: 0.590 \t\n",
      "\n",
      "  acc: 0.590 \t \n",
      "EarlyStopping counter: 1 out of 10\n",
      " training loss: 1.016 \t acc: 0.590 \t\n",
      "\n",
      "  acc: 0.591 \t \n",
      "EarlyStopping counter: 2 out of 10\n",
      " training loss: 1.013 \t acc: 0.592 \t\n",
      "\n",
      "  acc: 0.592 \t \n",
      "EarlyStopping counter: 3 out of 10\n",
      " training loss: 1.016 \t acc: 0.591 \t\n",
      "\n",
      "  acc: 0.592 \t \n",
      "EarlyStopping counter: 4 out of 10\n",
      " training loss: 1.015 \t acc: 0.590 \t\n",
      "\n",
      "  acc: 0.594 \t \n",
      "EarlyStopping counter: 5 out of 10\n",
      " training loss: 1.012 \t acc: 0.590 \t\n",
      "\n",
      "  acc: 0.590 \t \n",
      "EarlyStopping counter: 6 out of 10\n",
      " training loss: 1.010 \t acc: 0.592 \t\n",
      "\n",
      "  acc: 0.591 \t \n",
      "EarlyStopping counter: 7 out of 10\n",
      " training loss: 1.010 \t acc: 0.592 \t\n",
      "\n",
      "  acc: 0.590 \t \n",
      "EarlyStopping counter: 8 out of 10\n",
      " training loss: 1.009 \t acc: 0.593 \t\n",
      "\n",
      "  acc: 0.590 \t \n",
      "EarlyStopping counter: 9 out of 10\n",
      " training loss: 1.008 \t acc: 0.593 \t\n",
      "\n",
      "  acc: 0.592 \t \n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "epoch=200\n",
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    train_loss, train_size, train_acc = 0,0,0\n",
    "    for batch_idx, (rhorho_s,rhorho_t,label_s,label_t) in enumerate(train_loader):\n",
    "        opt_g.zero_grad()\n",
    "        outputs = model(rhorho_s)\n",
    "        loss = criterion(outputs,torch.argmax(label_s,axis=1))\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        train_acc += (predictions==torch.argmax(label_s,axis=1)).sum().item()\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()*len(rhorho_s)\n",
    "        train_size += len(rhorho_s)\n",
    "        opt_g.step()\n",
    "        print('\\r training loss: %.3f \\t acc: %.3f \\t'%(train_loss/train_size, train_acc/train_size), end='')\n",
    "    print()\n",
    "    vaild_acc, valid_size = 0,0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (rhorho_s,rhorho_t,label_s,label_t) in enumerate(valid_loader):\n",
    "            outputs = model(rhorho_s)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            vaild_acc += (predictions==torch.argmax(label_s,axis=1)).sum().item()\n",
    "            valid_size += len(rhorho_s)\n",
    "    print()\n",
    "    print('\\r  acc: %.3f \\t ' %(vaild_acc/valid_size),end='')\n",
    "    print()\n",
    "    early_stopping(-vaild_acc/valid_size,model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
